{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from scipy.stats import kde\n",
    "from torchvision.utils import make_grid\n",
    "from torch.autograd import grad\n",
    "import numpy as np \n",
    "import numpy.linalg as la \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import copy \n",
    "import time \n",
    "\n",
    "# set up device \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# plotting images \n",
    "def show(img):\n",
    "    npimg = img.cpu().numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVGD sampler to sample from p(x) known up to some constant. \n",
    "class SVGD_model():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def SVGD_kernel(self, x, h=-1):\n",
    "        init_dist = pdist(x)\n",
    "        pairwise_dists = squareform(init_dist)\n",
    "        if h < 0:  # if h < 0, using median trick\n",
    "            h = np.median(pairwise_dists)\n",
    "            h = h ** 2 / np.log(x.shape[0] + 1)\n",
    "\n",
    "        kernal_xj_xi = np.exp(- pairwise_dists ** 2 / h)\n",
    "        d_kernal_xi = np.zeros(x.shape)\n",
    "        for i_index in range(x.shape[0]):\n",
    "            d_kernal_xi[i_index] = np.matmul(kernal_xj_xi[i_index], x[i_index] - x) * 2 / h\n",
    "\n",
    "        return kernal_xj_xi, d_kernal_xi\n",
    "\n",
    "    def update(self, x0, dlnprob, n_iter=5000, stepsize=1e-3, bandwidth=-1, alpha=0.9, debug=False):\n",
    "        # Check input\n",
    "        if x0 is None or dlnprob is None:\n",
    "            raise ValueError('x0 or lnprob cannot be None!')\n",
    "        \n",
    "        x = np.copy(x0)\n",
    "\n",
    "        # adagrad with momentum\n",
    "        eps_factor = 1e-8\n",
    "        historical_grad_square = 0\n",
    "        for iter in range(n_iter):\n",
    "            if debug and (iter + 1) % 1000 == 0:\n",
    "                print('iter ' + str(iter + 1))\n",
    "\n",
    "            kernal_xj_xi, d_kernal_xi = self.SVGD_kernel(x, h=-1)\n",
    "            current_grad = (np.matmul(kernal_xj_xi, dlnprob(x)) + d_kernal_xi) / x.shape[0]\n",
    "            if iter == 0:\n",
    "                historical_grad_square += current_grad ** 2\n",
    "            else:\n",
    "                historical_grad_square = alpha * historical_grad_square + (1 - alpha) * (current_grad ** 2)\n",
    "            adj_grad = current_grad / np.sqrt(historical_grad_square + eps_factor)\n",
    "            x += stepsize * adj_grad\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# VAE class \n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, feature_size, latent_size, exp_family=True, M=5, Fisher=True):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_size = latent_size \n",
    "        \n",
    "        # encoder\n",
    "        self.enc = nn.Sequential(nn.Linear(feature_size, 512), nn.ReLU(True), \n",
    "                                 nn.Linear(512, 256), nn.ReLU(True))\n",
    "        self.enc1 = nn.Linear(256, latent_size)\n",
    "        self.enc2 = nn.Linear(256, latent_size)\n",
    "\n",
    "        # decoder\n",
    "        self.dec = nn.Sequential(nn.Linear(latent_size, 256), nn.ReLU(True), \n",
    "                                 nn.Linear(256, 512), nn.ReLU(True), nn.Linear(512, feature_size))\n",
    "        \n",
    "        # Exp. family prior/posterior \n",
    "        self.M = M\n",
    "        self.exp_coef = nn.Parameter(torch.randn(M, latent_size).normal_(0, 0.01))\n",
    "        \n",
    "        # Fisher/KL VAE \n",
    "        self.Fisher = Fisher \n",
    "        \n",
    "        # use exp_family model for prior\n",
    "        self.exp_family = exp_family\n",
    "        \n",
    "        # exp. family natural parameter/ sufficient statistic\n",
    "        self.natural_param = nn.Parameter(torch.randn(M*latent_size, 1).normal_(0, 0.01))\n",
    "        \n",
    "        # sufficient statistic \n",
    "        self.sufficient_stat = nn.Sequential(nn.Linear(latent_size, M*latent_size), nn.SELU(), \n",
    "                                 nn.Linear(M*latent_size, M*latent_size), nn.SELU(), nn.Linear(M*latent_size, M*latent_size),\n",
    "                                 nn.SELU(), nn.Linear(M*latent_size, M*latent_size), nn.SELU(),\n",
    "                                 nn.Linear(M*latent_size, M*latent_size))\n",
    "        \n",
    "    # Exp. family model     \n",
    "    def dlnpz_exp(self, z, polynomial=True):\n",
    "        '''\n",
    "        --- returns both dz log p(z) and p(z)\n",
    "        --- up to some multiplicative constant \n",
    "        '''\n",
    "        if polynomial == True:\n",
    "            c = self.exp_coef\n",
    "            dlnpz = 0\n",
    "            lnpz = 0\n",
    "            for m in range(self.M):\n",
    "                dlnpz += (m+1)*z**(m) * c[m,:].unsqueeze(0)\n",
    "                lnpz += z**(m+1) * c[m,:].unsqueeze(0)\n",
    "\n",
    "            pz = lnpz.sum(dim=1).exp()\n",
    "\n",
    "            return dlnpz, pz\n",
    "        else:\n",
    "            Tz = self.sufficient_stat(z)\n",
    "            eta = self.natural_param \n",
    "            lnpz = torch.mm(Tz, eta).sum()\n",
    "            dlnpz = grad(lnpz, z, retain_graph=True)[0]\n",
    "        \n",
    "            return dlnpz, lnpz.exp()\n",
    "            \n",
    "        \n",
    "    def encode(self, x):\n",
    "        h1 = self.enc(x)\n",
    "        mu_z = self.enc1(h1)\n",
    "        logvar_z = self.enc2(h1)\n",
    "        \n",
    "        return mu_z, logvar_z \n",
    "    \n",
    "    def decode(self, z):\n",
    "        h1 = self.dec(z)\n",
    "        x_hat = torch.sigmoid(h1)\n",
    "\n",
    "        return x_hat\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # encode \n",
    "        mu_z, logvar_z = self.encode(x) # input of the encoder \n",
    "        std_z = (0.5*logvar_z).exp() # std \n",
    "        q0 = torch.distributions.normal.Normal(mu_z, std_z) # dist. of epsilon N(0,1)\n",
    "        z = mu_z + std_z * torch.randn_like(std_z) # z ~ q(z|x)\n",
    "        \n",
    "        '''\n",
    "        Where normalizing flow should go!\n",
    "        z_ = f(z) \n",
    "        '''\n",
    "        \n",
    "        # decode \n",
    "        x_hat = self.decode(z)\n",
    "        \n",
    "        if self.Fisher is True:\n",
    "            dlnqzx = grad(q0.log_prob(z).sum(), x, create_graph=True)[0] # d/dx log q(z|x)\n",
    "            dlnqzz = grad(q0.log_prob(z).sum(), z, create_graph=True)[0] # d/dz log q(z|x)\n",
    "            stability = 0.5* dlnqzx.pow(2).sum() # stability term \n",
    "            pxz = torch.distributions.normal.Normal(x_hat, 1.0) # p(x|z)\n",
    "            lnpxz = pxz.log_prob(x) # log p(x|z)\n",
    "            dlnpxz = grad(lnpxz.sum(), z, retain_graph=True)[0] # d/dz log p(x|z)\n",
    "            \n",
    "            if self.exp_family is True:\n",
    "                dlnpz, _ = self.dlnpz_exp(z) # Exp. family prior \n",
    "            else:\n",
    "                dlnpz = -z # Gaussian prior \n",
    "                \n",
    "            fisher_div = 0.5*(dlnqzz - dlnpz - dlnpxz).pow(2).sum() # Fisher div. with one sample from q(z|x)\n",
    "            \n",
    "            return x_hat, fisher_div, stability \n",
    "        \n",
    "        else:\n",
    "            pz = torch.distributions.normal.Normal(0., 1.) # prior dist. \n",
    "            KL = q0.log_prob(z).sum() - pz.log_prob(z).sum() # KL[q(z|x) || p(z)]\n",
    "            \n",
    "            return x_hat, KL \n",
    "    \n",
    "    # the VAE loss function \n",
    "    def loss(self, x, output):\n",
    "        \n",
    "        if self.Fisher is True:\n",
    "            x_hat, fisher_div, stability = output \n",
    "            MSE = 0.5*(x-x_hat).pow(2).sum()\n",
    "            loss = fisher_div + MSE + stability \n",
    "        else:\n",
    "            x_hat, KL = output \n",
    "            MSE = 0.5*(x-x_hat).pow(2).sum()\n",
    "            # BCE = F.binary_cross_entropy(x_hat, x.detach(), reduction='sum')\n",
    "            loss = KL + MSE \n",
    "\n",
    "        return loss / x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FID score \n",
    "from fid import fid_score\n",
    "import fid.tools as tools\n",
    "from fid.inception import InceptionV3\n",
    "\n",
    "base_fid_statistics = None\n",
    "inception_model = None\n",
    "\n",
    "def initialize_fid(train_loader, sample_size=1000):\n",
    "    global base_fid_statistics, inception_model\n",
    "    if inception_model is None:\n",
    "        inception_model = InceptionV3([InceptionV3.BLOCK_INDEX_BY_DIM[2048]])\n",
    "    inception_model = tools.cuda(inception_model)\n",
    "\n",
    "    if base_fid_statistics is None:\n",
    "        train_images = []\n",
    "        for images, _ in train_loader:\n",
    "            train_images += list(images.numpy())\n",
    "            if len(train_images) > sample_size:\n",
    "                train_images = train_images[:sample_size]\n",
    "                break\n",
    "        train_images = np.array(train_images)\n",
    "        base_fid_statistics = fid_score.calculate_activation_statistics(\n",
    "            train_images, inception_model, cuda=tools.is_cuda_available(),\n",
    "            dims=2048)\n",
    "        inception_model.cpu() \n",
    "\n",
    "\n",
    "def fid(generated_images, noise=None):\n",
    "    score = fid_images(generated_images)\n",
    "    return score\n",
    "\n",
    "\n",
    "def fid_images(generated_images):\n",
    "    global base_fid_statistics, inception_model\n",
    "    inception_model = tools.cuda(inception_model)\n",
    "    m1, s1 = fid_score.calculate_activation_statistics(\n",
    "        generated_images.data.cpu().numpy(), inception_model, cuda=tools.is_cuda_available(),\n",
    "        dims=2048)\n",
    "    inception_model.cpu()\n",
    "    m2, s2 = base_fid_statistics\n",
    "    ret = fid_score.calculate_frechet_distance(m1, s1, m2, s2)\n",
    "    return ret\n",
    "\n",
    "# generate images from random input using SVGD \n",
    "def generate_images(model, num_samples, n_iter=16000, stepsize=1e-4):\n",
    "    if model.Fisher is True:\n",
    "        # SVGD samppling \n",
    "        def dlnp(z):\n",
    "            z = torch.tensor(z, dtype=torch.float, requires_grad=True)\n",
    "            dlnp, _ = model.dlnpz_exp(z)\n",
    "            return dlnp.detach().numpy()\n",
    "\n",
    "        svgd_sampler = SVGD_model()\n",
    "        z0 = np.random.rand(num_samples, model.latent_size)\n",
    "        # z0 = 2*torch.rand(num_samples, model.latent_size) - 1\n",
    "        # z0 = z0.to(device)\n",
    "        samples = svgd_sampler.update(x0=z0, dlnprob=dlnp, n_iter=n_iter, stepsize=stepsize)\n",
    "        \n",
    "        # decode samples \n",
    "        x_hat = model.decode(torch.tensor(samples, dtype=torch.float))\n",
    "        x_hat = x_hat.detach()\n",
    "        x_hat = x_hat.reshape(num_samples, 1, 28, 28)\n",
    "        \n",
    "    else:\n",
    "        z = torch.randn(num_samples, model.latent_size)\n",
    "        x_hat = model.decode(z)\n",
    "        x_hat = x_hat.detach()\n",
    "        x_hat = x_hat.reshape(num_samples, 1, 28, 28)\n",
    "        \n",
    "    return x_hat  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "batch_size = 100\n",
    "train_set,test_set,train_loader,test_loader = {},{},{},{}\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_set['mnist'] = torchvision.datasets.MNIST(root='~/data', train=True, download=False, transform=transform)\n",
    "test_set['mnist'] = torchvision.datasets.MNIST(root='~/data', train=False, download=False, transform=transform)\n",
    "train_loader['mnist'] = torch.utils.data.DataLoader(train_set['mnist'], batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader['mnist'] = torch.utils.data.DataLoader(test_set['mnist'], batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# optimizer\n",
    "def make_optimizer(optimizer_name, model, **kwargs):\n",
    "    if optimizer_name=='Adam':\n",
    "        optimizer = optim.Adam(model.parameters(),lr=kwargs['lr'], betas=[0.9, 0.999])\n",
    "    elif optimizer_name=='SGD':\n",
    "        optimizer = optim.SGD(model.parameters(),lr=kwargs['lr'],momentum=kwargs['momentum'], weight_decay=kwargs['weight_decay'])\n",
    "    elif optimizer_name == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(),lr=kwargs['lr'], momentum=0.9)\n",
    "    else:\n",
    "        raise ValueError('Not valid optimizer name')\n",
    "    return optimizer\n",
    "\n",
    "# scheduler \n",
    "def make_scheduler(scheduler_name, optimizer, **kwargs):\n",
    "    if scheduler_name=='MultiStepLR':\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer,milestones=kwargs['milestones'],gamma=kwargs['factor'])\n",
    "    else:\n",
    "        raise ValueError('Not valid scheduler name')\n",
    "    return scheduler\n",
    "\n",
    "# training parameters \n",
    "data_name = 'mnist'\n",
    "optimizer_name = 'Adam'\n",
    "scheduler_name = 'MultiStepLR'\n",
    "num_epochs = 20\n",
    "lr = 1e-3\n",
    "device = torch.device(device)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "\n",
    "# VAE \n",
    "local_vae = VAE(feature_size=784, latent_size=10, M=8, Fisher=True, exp_family=True).to(device)\n",
    "optimizer = make_optimizer(optimizer_name, local_vae, lr=lr , weight_decay=0)\n",
    "scheduler = make_scheduler(scheduler_name, optimizer, milestones=[50, 70, 90], factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the VAE \n",
    "loader = train_loader['mnist']\n",
    "for epoch in tqdm(range(num_epochs+1)):\n",
    "    loss_epoch = 0 \n",
    "    for data, _ in loader:\n",
    "        # zero grad \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass \n",
    "        data = Variable(data.reshape(data.shape[0], 784), requires_grad=True).to(device)\n",
    "        output = local_vae.forward(data)\n",
    "        loss = local_vae.loss(data, output)\n",
    "        loss_epoch += loss.item() \n",
    "        \n",
    "        # backward pass \n",
    "        loss.backward()\n",
    "        \n",
    "        # update parameters \n",
    "        optimizer.step()\n",
    "        \n",
    "    # print loss at the end of every epoch \n",
    "    print('Epoch : ', epoch, ' | Loss VAE: {:.4f}'.format(loss_epoch / len(loader)), ' | lr : ', optimizer.param_groups[0]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the prior then decode to generate new images \n",
    "x_hat_fisher = generate_images(model=local_vae.cpu(), num_samples=100, n_iter=16000, stepsize=1e-3)\n",
    "plt.figure()\n",
    "data_size = torch.Size([64, 1, 28, 28])\n",
    "show(make_grid(x_hat_fisher[0:64, :], padding=0))\n",
    "plt.title('Generated data (exp. prior)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute FID score \n",
    "initialize_fid(test_loader['mnist'], sample_size=10000)\n",
    "score_fisher = fid_images(x_hat_fisher)\n",
    "print(score_fisher)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
