{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from scipy.stats import kde\n",
    "from torchvision.utils import make_grid\n",
    "from torch.autograd import grad\n",
    "import numpy as np \n",
    "import numpy.linalg as la \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import copy \n",
    "import time \n",
    "\n",
    "# set up device \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# plotting images \n",
    "def show(img):\n",
    "    npimg = img.cpu().numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.SELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVGD sampler to sample from p(x) known up to some constant. \n",
    "class SVGD_model():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def SVGD_kernel(self, x, h=-1):\n",
    "        init_dist = pdist(x)\n",
    "        pairwise_dists = squareform(init_dist)\n",
    "        if h < 0:  # if h < 0, using median trick\n",
    "            h = np.median(pairwise_dists)\n",
    "            h = h ** 2 / np.log(x.shape[0] + 1)\n",
    "\n",
    "        kernal_xj_xi = np.exp(- pairwise_dists ** 2 / h)\n",
    "        d_kernal_xi = np.zeros(x.shape)\n",
    "        for i_index in range(x.shape[0]):\n",
    "            d_kernal_xi[i_index] = np.matmul(kernal_xj_xi[i_index], x[i_index] - x) * 2 / h\n",
    "\n",
    "        return kernal_xj_xi, d_kernal_xi\n",
    "\n",
    "    def update(self, x0, dlnprob, n_iter=5000, stepsize=1e-3, bandwidth=-1, alpha=0.9, debug=False):\n",
    "        # Check input\n",
    "        if x0 is None or dlnprob is None:\n",
    "            raise ValueError('x0 or lnprob cannot be None!')\n",
    "        \n",
    "        x = np.copy(x0)\n",
    "\n",
    "        # adagrad with momentum\n",
    "        eps_factor = 1e-8\n",
    "        historical_grad_square = 0\n",
    "        for iter in range(n_iter):\n",
    "            if debug and (iter + 1) % 1000 == 0:\n",
    "                print('iter ' + str(iter + 1))\n",
    "\n",
    "            kernal_xj_xi, d_kernal_xi = self.SVGD_kernel(x, h=-1)\n",
    "            current_grad = (np.matmul(kernal_xj_xi, dlnprob(x)) + d_kernal_xi) / x.shape[0]\n",
    "            if iter == 0:\n",
    "                historical_grad_square += current_grad ** 2\n",
    "            else:\n",
    "                historical_grad_square = alpha * historical_grad_square + (1 - alpha) * (current_grad ** 2)\n",
    "            adj_grad = current_grad / np.sqrt(historical_grad_square + eps_factor)\n",
    "            x += stepsize * adj_grad\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# VAE class \n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, feature_size, latent_size, exp_family=True, M=5, Fisher=True):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_size = latent_size \n",
    "        \n",
    "        # encoder\n",
    "        self.enc = nn.Sequential(nn.Linear(feature_size, 512), nn.ReLU(True), \n",
    "                                 nn.Linear(512, 256), nn.ReLU(True))\n",
    "        self.enc1 = nn.Linear(256, latent_size)\n",
    "        self.enc2 = nn.Linear(256, latent_size)\n",
    "\n",
    "        # decoder\n",
    "        self.dec = nn.Sequential(nn.Linear(latent_size, 256), nn.ReLU(True), \n",
    "                                 nn.Linear(256, 512), nn.ReLU(True), nn.Linear(512, feature_size))\n",
    "        \n",
    "        # Exp. family prior/posterior \n",
    "        self.M = M\n",
    "        self.exp_coef = nn.Parameter(torch.randn(M, latent_size).normal_(0, 0.01))\n",
    "        \n",
    "        # Fisher/KL VAE \n",
    "        self.Fisher = Fisher \n",
    "        \n",
    "        # use exp_family model for prior\n",
    "        self.exp_family = exp_family\n",
    "        \n",
    "        # exp. family natural parameter/ sufficient statistic\n",
    "        self.natural_param = nn.Parameter(torch.randn(M*latent_size, 1).normal_(0, 0.01))\n",
    "        \n",
    "        # sufficient statistic \n",
    "        self.sufficient_stat = nn.Sequential(nn.Linear(latent_size, M*latent_size), nn.SELU(), \n",
    "                                 nn.Linear(M*latent_size, M*latent_size), nn.SELU(), nn.Linear(M*latent_size, M*latent_size),\n",
    "                                 nn.SELU(), nn.Linear(M*latent_size, M*latent_size), nn.SELU(),\n",
    "                                 nn.Linear(M*latent_size, M*latent_size))\n",
    "        \n",
    "    # Exp. family model     \n",
    "    def dlnpz_exp(self, z, polynomial=True):\n",
    "        '''\n",
    "        --- returns both dz log p(z) and p(z)\n",
    "        --- up to some multiplicative constant \n",
    "        '''\n",
    "        if polynomial == True:\n",
    "            c = self.exp_coef\n",
    "            dlnpz = 0\n",
    "            lnpz = 0\n",
    "            for m in range(self.M):\n",
    "                dlnpz += (m+1)*z**(m) * c[m,:].unsqueeze(0)\n",
    "                lnpz += z**(m+1) * c[m,:].unsqueeze(0)\n",
    "\n",
    "            pz = lnpz.sum(dim=1).exp()\n",
    "\n",
    "            return dlnpz, pz\n",
    "        else:\n",
    "            Tz = self.sufficient_stat(z)\n",
    "            eta = self.natural_param \n",
    "            lnpz = torch.mm(Tz, eta).sum()\n",
    "            dlnpz = grad(lnpz, z, retain_graph=True)[0]\n",
    "        \n",
    "            return dlnpz, lnpz.exp()\n",
    "            \n",
    "        \n",
    "    def encode(self, x):\n",
    "        h1 = self.enc(x)\n",
    "        mu_z = self.enc1(h1)\n",
    "        logvar_z = self.enc2(h1)\n",
    "        \n",
    "        return mu_z, logvar_z \n",
    "    \n",
    "    def decode(self, z):\n",
    "        h1 = self.dec(z)\n",
    "        x_hat = torch.sigmoid(h1)\n",
    "\n",
    "        return x_hat\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # encode \n",
    "        mu_z, logvar_z = self.encode(x) # input of the encoder \n",
    "        std_z = (0.5*logvar_z).exp() # std \n",
    "        q0 = torch.distributions.normal.Normal(mu_z, std_z) # dist. of epsilon N(0,1)\n",
    "        z = mu_z + std_z * torch.randn_like(std_z) # z ~ q(z|x)\n",
    "        \n",
    "        '''\n",
    "        Where normalizing flow should go!\n",
    "        z_ = f(z) \n",
    "        '''\n",
    "        \n",
    "        # decode \n",
    "        x_hat = self.decode(z)\n",
    "        \n",
    "        if self.Fisher is True:\n",
    "            dlnqzx = grad(q0.log_prob(z).sum(), x, create_graph=True)[0] # d/dx log q(z|x)\n",
    "            dlnqzz = grad(q0.log_prob(z).sum(), z, create_graph=True)[0] # d/dz log q(z|x)\n",
    "            stability = 0.5* dlnqzx.pow(2).sum() # stability term \n",
    "            pxz = torch.distributions.normal.Normal(x_hat, 1.0) # p(x|z)\n",
    "            lnpxz = pxz.log_prob(x) # log p(x|z)\n",
    "            dlnpxz = grad(lnpxz.sum(), z, retain_graph=True)[0] # d/dz log p(x|z)\n",
    "            \n",
    "            if self.exp_family is True:\n",
    "                dlnpz, _ = self.dlnpz_exp(z) # Exp. family prior \n",
    "            else:\n",
    "                dlnpz = -z # Gaussian prior \n",
    "                \n",
    "            fisher_div = 0.5*(dlnqzz - dlnpz - dlnpxz).pow(2).sum() # Fisher div. with one sample from q(z|x)\n",
    "            \n",
    "            return x_hat, fisher_div, stability \n",
    "        \n",
    "        else:\n",
    "            pz = torch.distributions.normal.Normal(0., 1.) # prior dist. \n",
    "            KL = q0.log_prob(z).sum() - pz.log_prob(z).sum() # KL[q(z|x) || p(z)]\n",
    "            \n",
    "            return x_hat, KL \n",
    "    \n",
    "    # the VAE loss function \n",
    "    def loss(self, x, output):\n",
    "        \n",
    "        if self.Fisher is True:\n",
    "            x_hat, fisher_div, stability = output \n",
    "            MSE = 0.5*(x-x_hat).pow(2).sum()\n",
    "            loss = fisher_div + MSE + stability \n",
    "        else:\n",
    "            x_hat, KL = output \n",
    "            MSE = 0.5*(x-x_hat).pow(2).sum()\n",
    "            # BCE = F.binary_cross_entropy(x_hat, x.detach(), reduction='sum')\n",
    "            loss = KL + MSE \n",
    "\n",
    "        return loss / x.shape[0]\n",
    "    \n",
    "    # sampling from the prior using Langevin diffusion     \n",
    "    def sample_prior(self, iter_num=100, eps=1e-3, reject=True):\n",
    "        '''\n",
    "        --- iter_num : number of Langevin itertions\n",
    "        --- eps : step size \n",
    "        --- returns samples ~ p(z)\n",
    "        '''\n",
    "        \n",
    "        z = torch.randn(1, self.latent_size)\n",
    "        samples = z \n",
    "        \n",
    "        for iter in range(iter_num):\n",
    "            # Langevin update \n",
    "            dlnpz_init, pz_init = self.dlnpz_exp(z)\n",
    "            z_init = z \n",
    "            z_cand = z_init + eps*dlnpz_init + np.sqrt(2*eps)*torch.randn_like(z_init)\n",
    "            dlnpz_cand, pz_cand = self.dlnpz_exp(z_cand)\n",
    "            \n",
    "            # Metropolis-Hastings update (accep/reject)\n",
    "            if reject is True:\n",
    "                q_init = (-1/4/eps * (z_init - z_cand - eps*dlnpz_cand).pow(2).sum()).exp()\n",
    "                q_cand = (-1/4/eps * (z_cand - z_init - eps*dlnpz_init).pow(2).sum()).exp()\n",
    "                ratio = (pz_cand * q_init) / (pz_init * q_cand)\n",
    "                alpha = min(1, ratio)\n",
    "                u = np.random.rand()\n",
    "                if u < alpha:\n",
    "                    z = z_cand\n",
    "                    samples = torch.cat((samples, z_cand), dim=0)\n",
    "                \n",
    "            else:\n",
    "                samples = torch.cat((samples, z_cand), dim=0)\n",
    "                z = z_cand \n",
    "            \n",
    "        return samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FID score \n",
    "from fid import fid_score\n",
    "import fid.tools as tools\n",
    "from fid.inception import InceptionV3\n",
    "\n",
    "base_fid_statistics = None\n",
    "inception_model = None\n",
    "\n",
    "def initialize_fid(train_loader, sample_size=1000):\n",
    "    global base_fid_statistics, inception_model\n",
    "    if inception_model is None:\n",
    "        inception_model = InceptionV3([InceptionV3.BLOCK_INDEX_BY_DIM[2048]])\n",
    "    inception_model = tools.cuda(inception_model)\n",
    "\n",
    "    if base_fid_statistics is None:\n",
    "        train_images = []\n",
    "        for images, _ in train_loader:\n",
    "            train_images += list(images.numpy())\n",
    "            if len(train_images) > sample_size:\n",
    "                train_images = train_images[:sample_size]\n",
    "                break\n",
    "        train_images = np.array(train_images)\n",
    "        base_fid_statistics = fid_score.calculate_activation_statistics(\n",
    "            train_images, inception_model, cuda=tools.is_cuda_available(),\n",
    "            dims=2048)\n",
    "        inception_model.cpu() \n",
    "\n",
    "\n",
    "def fid(generated_images, noise=None):\n",
    "    score = fid_images(generated_images)\n",
    "    return score\n",
    "\n",
    "\n",
    "def fid_images(generated_images):\n",
    "    global base_fid_statistics, inception_model\n",
    "    inception_model = tools.cuda(inception_model)\n",
    "    m1, s1 = fid_score.calculate_activation_statistics(\n",
    "        generated_images.data.cpu().numpy(), inception_model, cuda=tools.is_cuda_available(),\n",
    "        dims=2048)\n",
    "    inception_model.cpu()\n",
    "    m2, s2 = base_fid_statistics\n",
    "    ret = fid_score.calculate_frechet_distance(m1, s1, m2, s2)\n",
    "    return ret\n",
    "\n",
    "# generate images from random input \n",
    "def generate_images(model, num_samples, n_iter=16000, stepsize=1e-4):\n",
    "    if model.Fisher is True:\n",
    "        # SVGD samppling \n",
    "        def dlnp(z):\n",
    "            z = torch.tensor(z, dtype=torch.float, requires_grad=True)\n",
    "            dlnp, _ = model.dlnpz_exp(z)\n",
    "            return dlnp.detach().numpy()\n",
    "\n",
    "        svgd_sampler = SVGD_model()\n",
    "        z0 = np.random.rand(num_samples, model.latent_size)\n",
    "        # z0 = 2*torch.rand(num_samples, model.latent_size) - 1\n",
    "        # z0 = z0.to(device)\n",
    "        samples = svgd_sampler.update(x0=z0, dlnprob=dlnp, n_iter=n_iter, stepsize=stepsize)\n",
    "        \n",
    "        # decode samples \n",
    "        x_hat = model.decode(torch.tensor(samples, dtype=torch.float))\n",
    "        x_hat = x_hat.detach()\n",
    "        x_hat = x_hat.reshape(num_samples, 1, 28, 28)\n",
    "        \n",
    "    else:\n",
    "        z = torch.randn(num_samples, model.latent_size)\n",
    "        x_hat = model.decode(z)\n",
    "        x_hat = x_hat.detach()\n",
    "        x_hat = x_hat.reshape(num_samples, 1, 28, 28)\n",
    "        \n",
    "    return x_hat  \n",
    "\n",
    "def generate_images_NF(model, flow, num_samples):\n",
    "    z0 = torch.randn(num_samples, model.latent_size).to(device)\n",
    "    samples, _ = flow.forward(z0)\n",
    "\n",
    "    # decode samples \n",
    "    x_hat = model.decode(samples)\n",
    "    x_hat = x_hat.detach()\n",
    "    x_hat = x_hat.reshape(num_samples, 1, 28, 28)\n",
    "        \n",
    "    return x_hat  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samling from the prior using Normalizing flows \n",
    "class PlanarFlow(nn.Module):\n",
    "    def __init__(self, d=2, init_sigma=0.01):\n",
    "        \"\"\"\n",
    "        d : latent space dimensnion \n",
    "        init_sigma : var of the initial parameters \n",
    "        \"\"\"\n",
    "        super(PlanarFlow, self).__init__()\n",
    "        self.d = d \n",
    "        self.u = nn.Parameter(torch.randn(1, d).normal_(0, init_sigma))\n",
    "        self.w = nn.Parameter(torch.randn(1, d).normal_(0, init_sigma))\n",
    "        self.b = nn.Parameter(torch.randn(1).fill_(0))\n",
    "        \n",
    "        \n",
    "    def forward(self, x, normalize_u=True):  \n",
    "        if isinstance(x, tuple):\n",
    "            z, sum_log_abs_det_jacobians = x\n",
    "        else:\n",
    "            z, sum_log_abs_det_jacobians = x, 0\n",
    "        \n",
    "        # normalize\n",
    "        wtu = (self.w @ self.u.t()).squeeze()\n",
    "        m_wtu = - 1 + torch.log1p(wtu.exp())\n",
    "        u_hat = self.u + (m_wtu - wtu) * self.w / (self.w @ self.w.t())\n",
    "        \n",
    "        # compute transform \n",
    "        # u_hat = self.u\n",
    "        arg = z @ self.w.t() + self.b\n",
    "        f_z = z + u_hat*torch.tanh(arg)\n",
    "\n",
    "        # update log prob.      \n",
    "        psi = self.w * (1-torch.tanh(arg)**2)\n",
    "        sum_log_abs_det_jacobians = sum_log_abs_det_jacobians + (1 + psi @ u_hat.t()).abs().squeeze().log()\n",
    "              \n",
    "        return f_z, sum_log_abs_det_jacobians\n",
    "    \n",
    "\n",
    "def NF_sampler(model, K=20, iter_num = 22000, batch_size = 100, lr=1e-5):\n",
    "    \n",
    "    '''\n",
    "    model : the pretrained VAE model \n",
    "    K : number of planar layers \n",
    "    iter_num : number of SGD iterations\n",
    "    batch_size : batch-size for training \n",
    "    lr : SGD learning rate \n",
    "    '''\n",
    "    \n",
    "    flow = nn.Sequential(*[PlanarFlow(d=model.latent_size) for _ in range(K)]).to(device)\n",
    "    # flow = torch.load('flow.pth', map_location=torch.device('cpu')).to(device)\n",
    "    optimizer_nf = optim.RMSprop(flow.parameters(), lr=lr, momentum=0.9)\n",
    "   \n",
    "    for iter in range(iter_num):\n",
    "            \n",
    "        # zero the gradient \n",
    "        optimizer_nf.zero_grad()\n",
    "        \n",
    "        # forward pass \n",
    "        z = torch.randn(batch_size, model.latent_size).to(device)\n",
    "        z_k, sum_log_abs_det_jacobians = flow.forward(z)\n",
    "        \n",
    "        # log prob. target dist. \n",
    "        _, p = model.dlnpz_exp(z_k)\n",
    "        log_p = p.log()\n",
    "        \n",
    "        # KL loss\n",
    "        log_q = -0.5*z.pow(2).sum(1) - sum_log_abs_det_jacobians\n",
    "        loss = (log_q - log_p).mean(dim=0)\n",
    "        \n",
    "        # check nan values \n",
    "        # if torch.isnan(loss).item() is True or torch.isinf(loss).item() is True or np.isnan(loss.item()) is True:\n",
    "        param = list(flow.parameters())[0].data\n",
    "        if torch.isnan(param).data.cpu().numpy()[0,0] == True:    \n",
    "            break\n",
    "        else:\n",
    "            flow_prev = copy.deepcopy(flow)\n",
    "        \n",
    "        # SGD step \n",
    "        loss.backward()\n",
    "        optimizer_nf.step()\n",
    "        \n",
    "        if iter % (200) == 0:\n",
    "            print('KL = ', loss.item(), '..... iter = ', iter)\n",
    "        \n",
    "    \n",
    "    return flow_prev       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "batch_size = 100\n",
    "train_set,test_set,train_loader,test_loader = {},{},{},{}\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_set['mnist'] = torchvision.datasets.MNIST(root='~/Dropbox/Fisher_VAE/data', train=True, download=False, transform=transform)\n",
    "test_set['mnist'] = torchvision.datasets.MNIST(root='~/Dropbox/Fisher_VAE/data', train=False, download=False, transform=transform)\n",
    "train_loader['mnist'] = torch.utils.data.DataLoader(train_set['mnist'], batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader['mnist'] = torch.utils.data.DataLoader(test_set['mnist'], batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# optimizer\n",
    "def make_optimizer(optimizer_name, model, **kwargs):\n",
    "    if optimizer_name=='Adam':\n",
    "        optimizer = optim.Adam(model.parameters(),lr=kwargs['lr'], betas=[0.9, 0.999])\n",
    "    elif optimizer_name=='SGD':\n",
    "        optimizer = optim.SGD(model.parameters(),lr=kwargs['lr'],momentum=kwargs['momentum'], weight_decay=kwargs['weight_decay'])\n",
    "    elif optimizer_name == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(),lr=kwargs['lr'], momentum=0.9)\n",
    "    else:\n",
    "        raise ValueError('Not valid optimizer name')\n",
    "    return optimizer\n",
    "\n",
    "# scheduler \n",
    "def make_scheduler(scheduler_name, optimizer, **kwargs):\n",
    "    if scheduler_name=='MultiStepLR':\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer,milestones=kwargs['milestones'],gamma=kwargs['factor'])\n",
    "    else:\n",
    "        raise ValueError('Not valid scheduler name')\n",
    "    return scheduler\n",
    "\n",
    "# training parameters \n",
    "data_name = 'mnist'\n",
    "optimizer_name = 'Adam'\n",
    "scheduler_name = 'MultiStepLR'\n",
    "num_epochs = 20\n",
    "lr = 1e-3\n",
    "device = torch.device(device)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "\n",
    "# VAE \n",
    "local_vae = VAE(feature_size=784, latent_size=10, M=8, Fisher=True, exp_family=True).to(device)\n",
    "optimizer = make_optimizer(optimizer_name, local_vae, lr=lr , weight_decay=0)\n",
    "scheduler = make_scheduler(scheduler_name, optimizer, milestones=[50, 70, 90], factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = make_optimizer('RMSprop', vae, lr=1e-5 , weight_decay=0)\n",
    "# optimizer.param_groups[0]['lr'] = 1e-6\n",
    "# local_vae = local_vae.to(device)\n",
    "\n",
    "loader = train_loader['mnist']\n",
    "for epoch in tqdm(range(num_epochs+1)):\n",
    "    loss_epoch = 0 \n",
    "    for data, _ in loader:\n",
    "        # zero grad \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass \n",
    "        data = Variable(data.reshape(data.shape[0], 784), requires_grad=True).to(device)\n",
    "        output = local_vae.forward(data)\n",
    "        loss = local_vae.loss(data, output)\n",
    "        loss_epoch += loss.item() \n",
    "        \n",
    "        # backward pass \n",
    "        loss.backward()\n",
    "        \n",
    "        # gradient clipping \n",
    "        # torch.nn.utils.clip_grad_value_(local_vae.parameters(), clip_value=0.00000001)\n",
    "        \n",
    "        # update parameters \n",
    "        optimizer.step()\n",
    "        \n",
    "    # print loss at the end of every epoch \n",
    "    print('Epoch : ', epoch, ' | Loss VAE: {:.4f}'.format(loss_epoch / len(loader)), ' | lr : ', optimizer.param_groups[0]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained models \n",
    "fisher = torch.load('FisherVAE.pth', map_location=torch.device('cpu')).to(device)\n",
    "vae = torch.load('VAE.pth', map_location=torch.device('cpu')).to(device)\n",
    "\n",
    "# save models \n",
    "# torch.save(flow, 'flow.pth')\n",
    "# torch.save(local_vae, 'VSE_fashion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "x_hat_fisher = generate_images(model=local_vae.cpu(), num_samples=300, n_iter=20000, stepsize=3*1e-3)\n",
    "\n",
    "for i in range(0):\n",
    "    print(i, '-th iteration')\n",
    "    new_hat = generate_images(model=fisher.cpu(), num_samples=100, n_iter=16000, stepsize=1e-3)\n",
    "    x_hat_fisher = torch.cat((x_hat_fisher, new_hat), dim=0)\n",
    "\n",
    "print('duration is : ', time.time()-t0)\n",
    "initialize_fid(test_loader['mnist'], sample_size=10000)\n",
    "score_fisher = fid_images(x_hat_fisher)\n",
    "print(score_fisher)\n",
    "plt.figure()\n",
    "data_size = torch.Size([64, 1, 28, 28])\n",
    "show(make_grid(x_hat_fisher[0:64, :], padding=0))\n",
    "plt.title('Generated data (exp. prior)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hat_vae = generate_images(model=vae.cpu(), num_samples=300, n_iter=16000, stepsize=1e-3)\n",
    "for i in range(0):\n",
    "    print(i, '-th iteration')\n",
    "    new_hat = generate_images(model=vae.cpu(), num_samples=100, n_iter=16000, stepsize=1e-3)\n",
    "    x_hat_vae = torch.cat((x_hat_vae, new_hat), dim=0)\n",
    "    \n",
    "# initialize_fid(test_loader['mnist'], sample_size=10000)\n",
    "score_fisher = fid_images(x_hat_vae)\n",
    "print(score_fisher)\n",
    "plt.figure()\n",
    "data_size = torch.Size([64, 1, 28, 28])\n",
    "show(make_grid(x_hat_vae[0:64, :], padding=0))\n",
    "plt.title('Generated data (KL)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_hat = generate_images(model=fisher.cpu(), num_samples=64, n_iter=15000, stepsize=5*1e-3)\n",
    "# sampling with normalizing flows\n",
    "flow_new = NF_sampler(model=fisher.to(device), K=60, iter_num=5000, batch_size=1000, lr=5*1e-2)\n",
    "x_hat = generate_images_NF(model=fisher.to(device), flow=flow.to(device), num_samples=64)\n",
    "#initialize_fid(train_loader['mnist'], sample_size=1000)\n",
    "#score_fisher = fid_images(x_hat)\n",
    "\n",
    "plt.figure()\n",
    "data_size = torch.Size([64, 1, 28, 28])\n",
    "show(make_grid(x_hat.reshape(data_size), padding=0))\n",
    "plt.title('Generated data (exp. prior)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = list(flow_new.parameters())[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hat = next(iter(test_loader['mnist']))[0]\n",
    "data_size = torch.Size([64, 1, 28, 28])\n",
    "show(make_grid(x_hat[0:64], padding=0))\n",
    "plt.title('Generated data (exp. prior)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = torch.tensor(float('nan'))\n",
    "if torch.isnan(xx).item() is True:\n",
    "    print('True')\n",
    "else:\n",
    "    print('False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.isnan(xx).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_fid(test_loader['mnist'], sample_size=1000)\n",
    "score_fisher = fid_images(x_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hat = generate_images(model=fisher.to('cpu'), num_samples=64, n_iter=15000, stepsize=1e-3)\n",
    "# initialize_fid(test_loader['mnist'], sample_size=10000)\n",
    "# score_vae = fid_images(x_hat)\n",
    "# print(score_vae)\n",
    "plt.figure()\n",
    "data_size = torch.Size([64, 1, 28, 28])\n",
    "show(make_grid(x_hat.reshape(data_size), padding=0))\n",
    "plt.title('Generated data (KL)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metropolis-Hastings sampling \n",
    "def metropolis_hastings(iter_num=1000):\n",
    "    '''\n",
    "    --- return samples ~ p(z)\n",
    "    '''\n",
    "    z_initial = torch.rand(1, vae.latent_size)\n",
    "    samples = z_initial \n",
    "    n_accept = 0\n",
    "    \n",
    "    for iter in range(iter_num):\n",
    "        z_candidate = z_initial + np.sqrt(0.2)*torch.randn_like(z_initial)\n",
    "        q_initial = torch.distributions.normal.Normal(z_initial, 1.0)\n",
    "        q_candidate = torch.distributions.normal.Normal(z_candidate, 1.0)\n",
    "        _, p_initial = vae.dlnpz_exp(z_initial)\n",
    "        _, p_candidate = vae.dlnpz_exp(z_candidate)\n",
    "\n",
    "        # compute acceptance probability \n",
    "        ratio = ((q_candidate.log_prob(z_initial).sum().exp() * p_candidate) / \n",
    "                 (q_initial.log_prob(z_candidate).sum().exp() * p_initial))\n",
    "        \n",
    "        # accept / reject \n",
    "        accept = min(1, ratio)\n",
    "        u = np.random.rand()\n",
    "        if accept > u:\n",
    "            samples = torch.cat((samples, z_candidate), dim=0)\n",
    "            z_initial = z_candidate\n",
    "            n_accept += 1 \n",
    "    \n",
    "    return samples, n_accept\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
